# **************************************************
# Copyright (c) 2025, Mayank Mishra
# **************************************************

datasets:
  - class_name: HuggingFaceDataset
    class_args:
      # edit accordingly
      data_path: mayank-mishra/glaive-code-assisstant-v3-20k
      input_key: question
      output_key: answer
    # just some metadata for internal use
    data_name: glaive-20k
    # data sampling ratio is meaningless when we have 1 dataset
    data_sampling_ratio: 1
    # to format input and output for training accordingly
    input_format: "input: __input__\noutput: "
    output_format: "__output__"
    max_input_tokens: 4096
    max_output_tokens: 4096

model_args:
  model_name: ibm-granite/granite-3b-code-base
  # padding free transformer needs a gpt_base model.
  # To convert granite models to this class and convert back after training,
  # take a look at the readme of this repo

random_args:
  # for replication of experiment (however, flash attention is non-deterministic so replication generally won't work)
  seed: 42

tuning_args:
  tuning_method: full_finetuning

save_args:
  save_path: checkpoints
  save_interval: 5000

training_parameters:
  # we will use 2 accelerators so our total samples seen during training is:
  # num_training_steps * micro_batch_size * gradient_accumulation_steps * data_parallel_size
  # = 20000 (1 epoch since the dataset we are using here has 20k samples exactly)
  # note: data_parallel_size = num_accelerators
  num_training_steps: 5000
  eval_during_training: false
  micro_batch_size: 2
  gradient_accumulation_steps: 1
  gradient_clipping: 1

optimizer_args:
  class_name: TorchAdamW
  class_args:
    lr: 1e-5
    weight_decay: 0.1
    betas:
      - 0.9
      - 0.95
    eps: 1e-10

lr_scheduler_args:
  # linear, cosine or exponential decay
  lr_decay_style: cosine
  # linear warmup
  num_warmup_steps: 200
  # final lr will be 0.1 * max lr (max lr is set in optimizer args)
  lr_decay_factor: 0.1

mixed_precision_args:
  dtype: bf16

distributed_args:
  # use ZeRO-3 for model sharding, saves most memory but needs more communication. this is fine since we are doing training on 2 accelerators and they are connected via NVLink
  stage: 3
  torch_compile: true
