GPTBaseForCausalLM(
  (transformer): GPTBaseModel(
    (wte): ParameterizedEmbedding(2048, 32)
    (embedding_dropout): Identity()
    (h): ModuleList(
      (0-7): 8 x Block(
        (ln_1): RMSNorm((32,), eps=1e-05, elementwise_affine=True)
        (sequence_mixer): Attention(
          (c_attn): ParameterizedLinear(in_features=32, out_features=64, bias=False)
          (c_proj): ParameterizedLinear(in_features=32, out_features=32, bias=False)
          (softmax_dropout): Identity()
          (dropout): Identity()
        )
        (ln_2): RMSNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp_block): MoE(
          (gate): ParameterizedLinear(in_features=32, out_features=8, bias=False)
          (c_fc): ParameterizedExperts(num_experts=8, in_features=32, out_features=256)
          (act): GLUActivation(
            (base_activation): SiLU()
          )
          (c_proj): ParameterizedExperts(num_experts=8, in_features=128, out_features=32)
          (dropout): Identity()
        )
      )
    )
    (ln_f): RMSNorm((32,), eps=1e-05, elementwise_affine=True)
    (rope): RoPE()
  )
  (lm_head): ParameterizedLinear(in_features=32, out_features=2048, bias=False)
)
Qwen2MoeForCausalLM(
  (model): Qwen2MoeModel(
    (embed_tokens): Embedding(2048, 32, padding_idx=2)
    (layers): ModuleList(
      (0-7): 8 x Qwen2MoeDecoderLayer(
        (self_attn): Qwen2MoeSdpaAttention(
          (q_proj): Linear(in_features=32, out_features=32, bias=False)
          (k_proj): Linear(in_features=32, out_features=16, bias=False)
          (v_proj): Linear(in_features=32, out_features=16, bias=False)
          (o_proj): Linear(in_features=32, out_features=32, bias=False)
          (rotary_emb): Qwen2MoeRotaryEmbedding()
        )
        (mlp): Qwen2MoeSparseMoeBlock(
          (gate): Linear(in_features=32, out_features=8, bias=False)
          (experts): ModuleList(
            (0-7): 8 x Qwen2MoeMLP(
              (gate_proj): Linear(in_features=32, out_features=1408, bias=False)
              (up_proj): Linear(in_features=32, out_features=1408, bias=False)
              (down_proj): Linear(in_features=1408, out_features=32, bias=False)
              (act_fn): SiLU()
            )
          )
          (shared_expert): Qwen2MoeMLP(
            (gate_proj): Linear(in_features=32, out_features=5632, bias=False)
            (up_proj): Linear(in_features=32, out_features=5632, bias=False)
            (down_proj): Linear(in_features=5632, out_features=32, bias=False)
            (act_fn): SiLU()
          )
          (shared_expert_gate): Linear(in_features=32, out_features=1, bias=False)
        )
        (input_layernorm): Qwen2MoeRMSNorm((32,), eps=1e-05)
        (post_attention_layernorm): Qwen2MoeRMSNorm((32,), eps=1e-05)
      )
    )
    (norm): Qwen2MoeRMSNorm((32,), eps=1e-05)
    (rotary_emb): Qwen2MoeRotaryEmbedding()
  )
  (lm_head): Linear(in_features=32, out_features=2048, bias=False)
)
