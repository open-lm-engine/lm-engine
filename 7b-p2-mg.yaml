# **************************************************
# Copyright (c) 2025, Mayank Mishra
# **************************************************

datasets:
  # class_name - data_name & data_sampling_ratio are not used but need to be passed to avoid errors
  - class_name: MegatronDataset
    data_name: Megatron
    data_sampling_ratio: 1
    class_args:
      eval_steps: 2
      fim_rate: 0.1
      data_cache_path: /mnt/vast/proj/checkpoints/granite-4-models-carina/data-cache
      # Option 1: data loading using --data-path with single file
      data_path:
        - 2.186918866200206
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/dclm-p1_8
        - 2.1798903078760477
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/dclm-p1_0
        - 2.18891737595724
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/dclm-p1_3
        - 2.188065853221436
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/dclm-p1_7
        - 2.1881899936871156
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/dclm-p1_9
        - 2.1919698142026878
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/dclm-p1_6
        - 2.19080171964259
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/dclm-p1_4
        - 2.193344227669897
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/dclm-p1_5
        - 2.1126524985249597
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/dclm-p1_2
        - 2.1846454278971814
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/dclm-p1_1
        - 1.5998456673029415
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/web-nemotron-cc-hq-p2_0
        - 1.594758247817697
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/web-nemotron-cc-hq-p2_1
        - 1.2501913337512325
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/web-nemotron-synthetic-hq-p2_1
        - 1.2493724720481947
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/web-nemotron-synthetic-hq-p2_2
        - 1.2502289697241518
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/web-nemotron-synthetic-hq-p2_3
        - 1.250207224476421
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/web-nemotron-synthetic-hq-p2_0
        - 30.0
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/math-g4-p2_0
        - 1.0
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/domain-g4-p1_0
        - 10.000002262385568
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/code-g4-p2_2
        - 9.999995717533201
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/code-g4-p2_1
        - 10.00000202008123
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/code-g4-p2_0
        - 0.9895480386168448
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/multilang-g4-p2_2
        - 0.992120311606616
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/multilang-g4-p2_0
        - 1.0183316497765396
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/multilang-g4-p2_1
        - 3.0
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/academia-g4-p1
        - 3.0
        - /mnt/vast/proj/datasets/granite-4-datasets-megatron-merged/technical-g4-p2_0
      split: 100,0,0
      sequence_length: 4096
      tokens:
        eos_token: 100257
        fim_pre: 100258
        fim_mid: 100259
        fim_suf: 100260
        fim_pad: 100261

tokenizer_args:
  tokenizer_name: /mnt/vast/proj/checkpoints/granite-4-models-carina/tokenizers/granite-4.0-tiktoken

kernel_args:
  kernels:
    - swiglu_unchunked_cute
    - rmsnorm_cute
    - scattermoe
    - mamba2_ssm
    - flash_attention_2
    - continuous_count_cute

model_args:
  model_class: AutoModelForCausalLM
  pretrained_config:
    initializer_range: 0.1
    layer_norm_epsilon: 1e-05
    model_type: gpt_dolomite
    normalization_function: rmsnorm
    position_embedding_type: nope
    hidden_size: 1536
    m_width: 6
    m_emb: 12
    m_residual: 0.22
    num_layers: 40
    init_method: mup
    tie_word_embeddings: true
    router_aux_loss_coef: 0.01
    bos_token_id: 100257
    eos_token_id: 100257
    pad_token_id: 100256
    max_position_embeddings: 4096
    vocab_size: 100352
    sequence_mixer_blocks:
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: softmax_attention
        num_attention_heads: 12
        num_key_value_heads: 4
        add_bias: false
        attention_multiplier: 0.0078125
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: softmax_attention
        num_attention_heads: 12
        num_key_value_heads: 4
        add_bias: false
        attention_multiplier: 0.0078125
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: softmax_attention
        num_attention_heads: 12
        num_key_value_heads: 4
        add_bias: false
        attention_multiplier: 0.0078125
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: softmax_attention
        num_attention_heads: 12
        num_key_value_heads: 4
        add_bias: false
        attention_multiplier: 0.0078125
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
      - sequence_mixer_type: mamba2
        activation_function: silu
        add_bias: false
        use_conv_bias: true
        num_heads: 48
        state_size: 128
        intermediate_size: 3072
        conv_kernel_size: 4
        num_groups: 1
        chunk_size: 256
    mlp_blocks:
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
      - mlp_type: MoE
        activation_function: swiglu
        intermediate_size: 512
        add_bias: false
        shared_intermediate_size: 1024
        num_experts: 64
        num_experts_per_tok: 6
  efficient_initialization: true

tuning_args:
  tuning_method: pretraining

save_args:
  save_path: /mnt/vast/proj/checkpoints/granite-4-models-carina/ckpts/7b-p2
  save_interval: 25000

load_args:
  load_path: /mnt/vast/proj/checkpoints/granite-4-models-carina/ckpts/7b-p1
  load_dataloader_state: false
  load_lr_scheduler: true
  load_starting_iteration: false
  load_experiments_tracker_state: false
  resume_learning_rate: true

logging_args:
  log_interval: 10
  experiments_tracker_name: wandb
  wandb_args:
    entity: ai-models
    project: granite-4-models
    name: g4-7b-p2-cw

training_parameters:
  num_training_steps: 260000
  eval_interval: 1000000000
  micro_batch_size: 10
  gradient_accumulation_steps: 1
  eval_during_training: false

optimizer_args:
  params_group_method: mup
  class_name: TorchAdamW
  class_args:
    lr: 0.01
    weight_decay: 0.1
    betas:
      - 0.9
      - 0.95
    eps: 1e-10

lr_scheduler_args:
  lr_decay_style: power
  num_warmup_steps: 1
  num_constant_steps: 0
  num_decay_steps: 259999
  extra_lr_scheduler_args:
    # 4 * global_batch_size
    a: 11520
    # constant
    b: -0.51
    # global_batch_size in number of tokens
    c: 11796480

mixed_precision_args:
  dtype: bf16

distributed_args:
  fsdp_algorithm: 2
  torch_compile: true
  tensor_parallel_world_size: 1
  # dispatching_dataloader: true
  zero_topology:
    data_parallel_sharding_world_size: 4
    data_parallel_replication_world_size: 72
